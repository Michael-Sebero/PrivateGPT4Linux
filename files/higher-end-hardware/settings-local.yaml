server:
  env_name: ${APP_ENV:local}

llm:
  mode: llamacpp
  # Should be matching the selected model
  max_new_tokens: 4096
  context_window: 3900
  tokenizer: NousResearch/Nous-Hermes-Llama2-13b

llamacpp:
 prompt_style: "llama2"
 llm_hf_repo_id: TheBloke/Nous-Hermes-Llama2-GGUF
 llm_hf_model_file: nous-hermes-llama2-13b.Q5_K_S.gguf

embedding:
  mode: huggingface

huggingface:
  embedding_hf_model_name: BAAI/bge-small-en-v1.5

vectorstore:
  database: qdrant

qdrant:
  path: local_data/private_gpt/qdrant
